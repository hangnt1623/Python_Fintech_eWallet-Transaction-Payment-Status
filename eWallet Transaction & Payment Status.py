# -*- coding: utf-8 -*-
"""Ver 2_K35_Thúy Hằng_Project 2_Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ps7Xuo6hCrdeOoPwFxO6TMZ4qbwfKhte

#**Import dataset & libraries**
"""

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/da/Python/ThuyHang_Python_Project_2/'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
warnings.filterwarnings("ignore")

#read_CSV
payment = pd.read_csv(path+"payment_report.csv", encoding='utf_8')
product = pd.read_csv(path+"product.csv", encoding='utf_8')
transactions= pd.read_csv(path+"transactions.csv", encoding='utf_8')

!pip install ydata-profiling

from ydata_profiling import ProfileReport

#Merge payment & product -> payment_enriched
payment_enriched = pd.merge(payment, product, on='product_id', how='left')

payment.shape

product.shape

transactions.shape

payment

product

transactions

payment.info()

product.info()

transactions.info()

"""# **EDA**

##**EDA payment_enriched**

###**Intial look**
"""

#Size payment_enriched
'Size payment_enriched: ', payment_enriched.shape

#payment_enriched
payment_enriched.info()

#payment_enriched
payment_enriched.head(10)

#payment_enriched
payment_enriched.describe()

"""###**Inspection & Validation**"""

#Overview initial
profile = ProfileReport(payment_enriched, title="EDA payment_enriched Report", explorative=True)
profile.to_notebook_iframe()

"""####Data types
- Change datatype of report_month to DATETIME
- Change datatype of payment_group, category, team_own to CATEGORY -> can be classified
"""

#dtype
payment_enriched.dtypes

#change dtype
payment_enriched['report_month'] = pd.to_datetime(payment_enriched['report_month'])
payment_enriched["payment_group"] = payment_enriched["payment_group"].astype("category")
payment_enriched['category'] = payment_enriched['category'].astype("category")
payment_enriched["team_own"] = payment_enriched["team_own"].astype("category")

#check dtype again

"""####Missing Value
- category missing data -> add "Unknown"
- team_own missing data -> add "Uncategorized"
"""

payment_enriched.isnull().sum()

#category
# add 'Unknown'
payment_enriched['category'] = payment_enriched['category'].cat.add_categories('Unknown')

# fillna
payment_enriched['category'].fillna('Unknown', inplace=True)

#team_own
# add 'Unassigned'
payment_enriched['team_own'] = payment_enriched['team_own'].cat.add_categories('Unassigned')
# fillna
payment_enriched['team_own'].fillna('Unassigned', inplace=True)

#check
payment_enriched.isnull().sum()

"""####Unique Value"""

payment_enriched.nunique()

"""####Duplicate Value
Duplicates: 0 row -> Next step: No action
"""

payment_enriched.duplicated().sum()

"""####Outliers"""

Q1 = payment_enriched['volume'].quantile(0.25)
Q3 = payment_enriched['volume'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

payment_enriched_outliers = payment_enriched[(payment_enriched['volume'] < lower_bound) | (payment_enriched['volume'] > upper_bound)]
print(payment_enriched_outliers)

"""####Distribution"""

#DF Payment
# Keep volume > 0 in order to keep 'log' no error
payment_enriched = payment_enriched[payment_enriched['volume'] > 0]

# Histogram log(1 + volume)
plt.figure(figsize=(10, 6))
plt.hist(np.log1p(payment_enriched['volume']), bins=50, color='yellow', edgecolor='black')

plt.title('Log-Scaled Distribution of Volume')
plt.xlabel('log(1 + Volume)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

#Overview last time
profile = ProfileReport(payment_enriched, title="EDA payment_enriched Report", explorative=True)
profile.to_notebook_iframe()

"""##**EDA transactions**

###**Initial look**
"""

#Size transactions
'Size transactions: ', transactions.shape

#transactions
transactions.info()

#transactions
transactions.head(10)

#transactions
transactions.describe()

"""###**Inspection & Validation**"""

#Overview initial
profile = ProfileReport(transactions, title="EDA Transactions Report", explorative=True)
profile.to_notebook_iframe()

"""####Data types
- Change datatype of transType & transStatus -> category -> classification
- Change datatype of sender_id & receiver_id -> int64 -> id in integer form
- Change datatype of extra_info -> string -> explain the information to be filled in
- Change datatype of timeStamp -> datetime because timeStamp is time
"""

#transactions
transactions.dtypes

#transType -> change dtype
transactions["transType"] = transactions["transType"].astype("category")
transactions["transStatus"] = transactions["transStatus"].astype("category")
transactions['sender_id'] = transactions['sender_id'].astype('Int64')
transactions['receiver_id'] = transactions['receiver_id'].astype('Int64')
transactions['extra_info'] = transactions['extra_info'].astype('string')
transactions['timeStamp'] = pd.to_datetime(transactions['timeStamp'], unit='ms')

transactions.dtypes

"""####Missing Value
- sender_id, receiver_id are missing data -> fill -1 to red flag
- extra_info has no data -> may be no additional information -> fill "No"
"""

#transactions
transactions.isnull().sum()

transactions['sender_id'].fillna(-1, inplace=True)
transactions['receiver_id'].fillna(-1, inplace=True)
transactions['extra_info'].fillna('No', inplace=True)

#check again
transactions.isna().sum()

"""####Unique Value"""

#transactions
transactions.nunique()

"""####Duplicate Value
There are 28 duplicated rows -> Next step: delete these 28 rows
"""

#transactions
transactions.duplicated().sum()

#Remove Duplicate
transactions.drop_duplicates(inplace=True)

#Check again
transactions.duplicated().sum()

"""####Outliers"""

Q1 = transactions['volume'].quantile(0.25)
Q3 = transactions['volume'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

transactions_outliers = transactions[(transactions['volume'] < lower_bound) | (transactions['volume'] > upper_bound)]
print(transactions_outliers)

"""####Distribution"""

#DF Transactions
# Keep volume > 0 in order to keep 'log' no error
transactions = transactions[transactions['volume'] > 0]

# Histogram log(1 + volume)
plt.figure(figsize=(10, 6))
plt.hist(np.log1p(transactions['volume']), bins=50, color='pink', edgecolor='black')

plt.title('Log-Scaled Distribution of Volume')
plt.xlabel('log(1 + Volume)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""# **Data Wrangling**

##**1. Top 3 product_ids with the highest volume.**
"""

#Top 3 product_ids with the highest volume
top_3_products = (
    payment_enriched
    .groupby('product_id')['volume']
    .sum()
    .sort_values(ascending=False)
    .head(3)
)

top_3_products

"""##**2. Given that 1 product_id is only owed by 1 team, are there any abnormal products against this rule?**"""

## Step 1: Count unique team_own per product_id
product_team_counts = payment_enriched.groupby('product_id')['team_own'].nunique()

## Step 2: Filter product_ids with more than 1 owning team
abnormal_products = product_team_counts[product_team_counts > 1].index

## Step 3: Show full records of abnormal products
payment_enriched[payment_enriched['product_id'].isin(abnormal_products)]

"""##**3. Find the team has had the lowest performance (lowest volume) since Q2.2023. Find the category that contributes the least to that team.**

"""

## Step 1: Filter data Q2/2023
df_q2_2023 = payment_enriched[payment_enriched['report_month'] >= '2023-04']
df_q2_2023

#lowest performance -> team?
lowest_performance_team = payment_enriched[payment_enriched['report_month'] >= '2023-04'].groupby(by = 'team_own')['volume'].sum().sort_values(ascending = True).head(1)
lowest_performance_team

#category contribute the least?
worst_cate = payment_enriched[(payment_enriched['report_month'] >= '2023-04') & (payment_enriched['team_own'] == 'APS')].groupby(by = 'category')['volume'].sum().sort_values(ascending = True).head(1)
worst_cate

"""##**4. Find the contribution of source_ids of refund transactions (payment_group = ‘refund’), what is the source_id with the highest contribution?**"""

# Filter refund transactions
df_refund = payment_enriched[payment_enriched['payment_group'] == 'refund']

# Calculate total volume by source_id
refund_contribution = df_refund.groupby('source_id')['volume'].sum()

# Find source_id with largest volume
top_source_id = refund_contribution.idxmax()
top_volume = refund_contribution.max()

# Calculate the contribution % of each source_id
refund_contribution_pct = refund_contribution / refund_contribution.sum() * 100

print(f"Source_id with highest refund contribution: {top_source_id}")
print(f"Volume: {top_volume:,}")
print(f"Contribution: {refund_contribution_pct[top_source_id]:.2f}%")

"""## 5. **Define type of transactions (‘transaction_type’) for each row, given:**
- transType = 2 & merchant_id = 1205: Bank Transfer Transaction
- transType = 2 & merchant_id = 2260: Withdraw Money Transaction
- transType = 2 & merchant_id = 2270: Top Up Money Transaction
- transType = 2 & others merchant_id: Payment Transaction
- transType = 8, merchant_id = 2250: Transfer Money
Transaction
- transType = 8 & others merchant_id: Split Bill Transaction
- Remained cases are invalid transactions
"""

conditions = [
    (transactions['transType'] == 2) & (transactions['merchant_id'] == 1205),
    (transactions['transType'] == 2) & (transactions['merchant_id'] == 2260),
    (transactions['transType'] == 2) & (transactions['merchant_id'] == 2270),
    (transactions['transType'] == 2),
    (transactions['transType'] == 8) & (transactions['merchant_id'] == 2250),
    (transactions['transType'] == 8)
]

transaction_types = [
    'Bank Transfer Transaction',
    'Withdraw Money Transaction',
    'Top Up Money Transaction',
    'Payment Transaction',
    'Transfer Money Transaction',
    'Split Bill Transaction'
]

transactions['transaction_type'] = np.select(conditions, transaction_types, default='Invalid Transaction')

"""##**6. Of each transaction type (excluding invalid transactions): find the number of transactions, volume, senders and receivers**"""

## Filter out invalid transactions
valid_df = transactions[transactions['transaction_type'] != 'Invalid Transaction']

## Group and aggregate
summary = valid_df.groupby('transaction_type').agg(
    num_transactions=('transaction_id', 'count'),
    total_volume=('volume', 'sum'),
    num_senders=('sender_id', pd.Series.nunique),
    num_receivers=('receiver_id', pd.Series.nunique)
).reset_index()

summary